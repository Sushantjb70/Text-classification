# Text-classification
Text classification using various feature engineering methods and applying various algorithms like logistic, SVM, Bagging and Boosting methods adn also with Neural networks.

# Steps
1. Dataset Preparation: The first step is the Dataset Preparation step which includes the process of loading a dataset and performing basic pre-processing. The dataset is then splitted into train and validation sets.
2. Feature Engineering: The next step is the Feature Engineering in which the raw dataset is transformed into flat features which can be used in a machine learning model. This step also includes the process of creating new features from the existing data.
3. Model Training: The final step is the Model Building step in which a machine learning model is trained on a labelled dataset.

# We will use the following methods for feature engineering.
1 Count Vectors as features <br />
2 TF-IDF Vectors as features <br />
  2.1 Word level <br />
  2.2 N-Gram level <br />
  2.3 Character level <br />
3 Word Embeddings as features <br />
4 Text / NLP based features <br />
5 Topic Models as features <br />

# We will use the follwing classifiers:
1 Naive Bayes Classifier
2 Linear Classifier
3 Support Vector Machine
4 Bagging Models
5 Boosting Models
6 Shallow Neural Networks
7 Convolutional Neural Network (CNN)
8 Long Short Term Modelr (LSTM)
9 Gated Recurrent Unit (GRU)
10 Bidirectional RNN
11 Recurrent Convolutional Neural Network (RCNN)

The data is in corpus1.txt file.The data contains amazon reviews.
